{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polynote/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import sys\n",
    "import re\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, MWETokenizer\n",
    "from pyvi import ViTokenizer\n",
    "from unicodedata import normalize\n",
    "pd.set_option('max_colwidth', -1)\n",
    "from hdfs import InsecureClient\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import hamming_loss,  accuracy_score, multilabel_confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InsecureClient('http://mt02.up.vn:50070')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_hdfs(path_file, sep):\n",
    "    with client.read(path_file, encoding='utf-8') as reader:\n",
    "        df_ = pd.read_csv(reader, delimiter=sep)\n",
    "        return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file_02 = '/user/anhdt157/politic_fb_post_clean/2006/part-00000-924c1669-1b15-40c8-9f9b-99747f77ea9c-c000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = read_csv_hdfs(path_file_02, '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xử lý dữ liệu bị trùng "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['clean_content']=df_test['clean_content'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dup(text):\n",
    "    return re.sub(r'(_) ','',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['clean_content'] = df_test['clean_content'].apply(lambda x: clean_dup(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_first50w(text):\n",
    "    text = text.split()[:50]\n",
    "    return ' '.join(text)\n",
    "def remove_last50w(text):\n",
    "    text = text.split()[-50:]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polynote/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/polynote/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_new = df_test[0:500000]\n",
    "df_new['remove_first50w'] = df_new['clean_content'].apply(lambda x: remove_first50w(x))\n",
    "df_new['remove_last50w'] = df_new['clean_content'].apply(lambda x: remove_last50w(x))\n",
    "df_new = df_new.drop_duplicates(subset=['remove_first50w'], keep ='first')\n",
    "df_new = df_new.drop_duplicates(subset=['remove_last50w'], keep ='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gán nhãn cho dữ liệu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword = pd.read_csv('keyword_label.txt',sep = '|',error_bad_lines=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_keyword['label'].tolist()\n",
    "keywords = df_keyword['clean_key'].apply(lambda x: x.split(\",\")).tolist()\n",
    "thresholds = df_keyword['threshold_keyword'].to_list()\n",
    "except_keywords = df_keyword['keyword_except'].apply(lambda x: x.split(\",\")).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 5), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = ','.join(df_keyword['clean_key']) + ','.join(df_keyword['keyword_except'])\n",
    "vocabulary = list({vocabulary})\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=(1,5))\n",
    "vectorizer.fit(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(df,labels, keywords,thresholds,except_keywords):\n",
    "    '''Gán nhãn cho bài viết : bài viết xuất hiện số từ trong tập keyword > threshold và không xuất hiện\n",
    "    từ trong nhóm keyword_except'''\n",
    "    test = vectorizer.transform(df)\n",
    "    data = pd.DataFrame.sparse.from_spmatrix(test)\n",
    "    data.columns = vectorizer.get_feature_names()\n",
    "    for label, keyword,threshold,except_keyword in zip(labels, keywords,thresholds,except_keywords):\n",
    "        data['keyword'] = data.loc[:,data.columns.intersection(keyword)].sum(axis=1)\n",
    "        data['keyword_except'] = data.loc[:,data.columns.intersection(except_keyword)].sum(axis=1)\n",
    "        data['sum_keyword'] = data['keyword'] - 1000*data['keyword_except']\n",
    "        data[label] = data['sum_keyword'].apply(lambda x:1 if x >= threshold else 0)\n",
    "        label_result = data[data.columns.intersection(labels)]\n",
    "    return label_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 10s, sys: 10.4 s, total: 5min 20s\n",
      "Wall time: 5min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "''' Gán nhãn dữ liệu. data final là tập tữ liệu đã được gán nhãn, sum_label là tổng số nhãn được\n",
    "gán của bài vieets'''\n",
    "df_labeling = labeling(df_new['clean_content'], labels, keywords,thresholds,except_keywords)\n",
    "df_new.reset_index(drop=True, inplace=True)\n",
    "df_labeling.reset_index(drop=True, inplace=True)\n",
    "data_final = pd.concat([df_new,df_labeling],axis=1)\n",
    "data_final['sum_label']=data_final.iloc[:,8:20].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Chỉ train cho tập dữ liệu được gán ít nhất 1 nhãn'''\n",
    "df_train = data_final.loc[data_final['sum_label']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train['clean_content']\n",
    "y = df_train.iloc[:,8:20]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 s, sys: 880 ms, total: 47.7 s\n",
      "Wall time: 47.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(X)\n",
    "tfidf_train = tfidf.transform(x_train)\n",
    "tfidf_test = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 44min 21s, sys: 30.5 s, total: 1h 44min 51s\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify = OneVsRestClassifier(XGBClassifier(learning_rate=0.1,max_depth=3 ))\n",
    "classify.fit(tfidf_train,y_train)\n",
    "y_pred = classify.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classify, open('model_train.pkl','wb'))\n",
    "pickle.dump(tfidf, open('tfidt_train.pkl','wb'))\n",
    "pickle.dump(vectorizer, open('vectorize_keyword_train.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming loss 0.016454974759811107\n",
      "accuracy 0.8243282852955545\n",
      "micro avg 0.92431845416417\n",
      "macro avg 0.9148980450881478\n"
     ]
    }
   ],
   "source": [
    "print(\"hamming loss {}\".format(hamming_loss(y_test,y_pred)))\n",
    "print(\"accuracy {}\".format(accuracy_score(y_test,y_pred)))\n",
    "print(\"micro avg {}\".format(f1_score(y_test,y_pred, average='micro')))\n",
    "print(\"macro avg {}\".format(f1_score(y_test,y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[19641    28]\n",
      "  [   44   757]]\n",
      "\n",
      " [[18408    96]\n",
      "  [  153  1813]]\n",
      "\n",
      " [[17252   120]\n",
      "  [  196  2902]]\n",
      "\n",
      " [[16767   193]\n",
      "  [  832  2678]]\n",
      "\n",
      " [[17541    14]\n",
      "  [  177  2738]]\n",
      "\n",
      " [[20028     9]\n",
      "  [   47   386]]\n",
      "\n",
      " [[17173    62]\n",
      "  [  613  2622]]\n",
      "\n",
      " [[13807   325]\n",
      "  [  743  5595]]\n",
      "\n",
      " [[20024    47]\n",
      "  [   65   334]]\n",
      "\n",
      " [[16367    31]\n",
      "  [   43  4029]]\n",
      "\n",
      " [[19592    43]\n",
      "  [  123   712]]\n",
      "\n",
      " [[20315    19]\n",
      "  [   19   117]]]\n"
     ]
    }
   ],
   "source": [
    "print(multilabel_confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'infor choi jin hyuk are coming back as a zombie mong_chờ tạo_hình xác sống của jin hyuk quá đi choi jin hyuk và park_joo hyun sẽ tham_gia cho một bộ phim zombie mới trên kbs có tên là thám_tử zombie zombie dectective bộ phim kể về một thây_ma sống lại năm thứ hai và trở_thành thám_tử tìm về quá_khứ cố hết_sức để cùng tồn_tại với con_người choi jin hyuk sẽ vào_vai kim moo young một thây_ma đẹp_trai với vóc_dáng lạ_thường đã hai năm kể từ khi anh ta sống lại như một thây_ma và bị mất trí_nhớ anh cố_gắng hoàn_thiện cách nói và cách đi_đứng từ một zombie vụng_về và làm lại cuộc_đời với tư_cách là một thám_tử park_joo hyun sẽ đảm_nhận vai gong seon ji một người viết tin của một chương_trình điều_tra cô có tinh_thần lạc_quan kiên_cường và luôn muốn đấu_tranh vì công_lý khi một nhân_chứng của vụ án mà cô đang điều_tra bị tấn_công bởi một kẻ không rõ danh_tính cô đã rời ngành điều_tra sau đó cô gặp thám_tử zombie kim moo young và bắt_đầu làm_việc tại văn_phòng của anh ta thám_tử zombie zombie sẽ được sản_xuất bởi đội_ngũ nhà_sản_xuất các bộ phim the producers go back couple được đạo_diễn bởi shim_jae hyun đồng_đạo diễn của the producers và được viết bởi baek eun jin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chính trị quôc tế</th>\n",
       "      <th>Chủ quyền</th>\n",
       "      <th>Luật pháp - tố tụng</th>\n",
       "      <th>Nhân vật chính trị</th>\n",
       "      <th>Sai phạm</th>\n",
       "      <th>Tôn giáo - tín ngưỡng</th>\n",
       "      <th>Tư tưởng tiêu cực - chống phá nhà nước</th>\n",
       "      <th>Tổ chức nhà nước</th>\n",
       "      <th>Tổ chức nhân sự</th>\n",
       "      <th>Y tế</th>\n",
       "      <th>khác</th>\n",
       "      <th>Đất đai</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chính trị quôc tế  Chủ quyền  Luật pháp - tố tụng  Nhân vật chính trị  \\\n",
       "0  0                  0          1                    0                    \n",
       "\n",
       "   Sai phạm  Tôn giáo - tín ngưỡng  Tư tưởng tiêu cực - chống phá nhà nước  \\\n",
       "0  0         0                      0                                        \n",
       "\n",
       "   Tổ chức nhà nước  Tổ chức nhân sự  Y tế  khác  Đất đai  \n",
       "0  0                 0                0     0     0        "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeling([a],labels, keywords,thresholds,except_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
